{
 "cells": [
  {
   "metadata": {},
   "cell_type": "raw",
   "source": "",
   "id": "7f800ba08ffda0db"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T11:33:14.782705Z",
     "start_time": "2025-03-26T11:33:14.752780Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import re\n",
    "from db.queries import get_sample_rows, get_user_forecast_data, row_to_config, get_predictive_data\n",
    "import os"
   ],
   "id": "ea8ab455299c5193",
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-26T11:33:14.798404Z",
     "start_time": "2025-03-26T11:33:14.785692Z"
    }
   },
   "source": "os.environ[\"ENV\"] = \"DEV\"",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T11:33:16.240518Z",
     "start_time": "2025-03-26T11:33:14.972415Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ufm_df = get_user_forecast_data(databrick_task_id=67)\n",
    "ufm_df.head()"
   ],
   "id": "36a75ac455bebc42",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "    StartDate     EndDate         Parameters Region  Status  ForecastMethodID  \\\n",
       "0  2025-02-01  2029-01-31  (5,2,0.3,0.3,0.8)     EC  Failed                 6   \n",
       "\n",
       "   UserForecastMethodID                                       CustomerJSON  \\\n",
       "0                   199  {\"CSA\": [\"\",\"Kimberley\",\"Klerksdorp\",\"South Af...   \n",
       "\n",
       "                                             varJSON   Method  DatabrickID  \n",
       "0  {\"VariableID\": [\"OffPeakConsumption\",\"PeakCons...  XGBoost           67  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>StartDate</th>\n",
       "      <th>EndDate</th>\n",
       "      <th>Parameters</th>\n",
       "      <th>Region</th>\n",
       "      <th>Status</th>\n",
       "      <th>ForecastMethodID</th>\n",
       "      <th>UserForecastMethodID</th>\n",
       "      <th>CustomerJSON</th>\n",
       "      <th>varJSON</th>\n",
       "      <th>Method</th>\n",
       "      <th>DatabrickID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-02-01</td>\n",
       "      <td>2029-01-31</td>\n",
       "      <td>(5,2,0.3,0.3,0.8)</td>\n",
       "      <td>EC</td>\n",
       "      <td>Failed</td>\n",
       "      <td>6</td>\n",
       "      <td>199</td>\n",
       "      <td>{\"CSA\": [\"\",\"Kimberley\",\"Klerksdorp\",\"South Af...</td>\n",
       "      <td>{\"VariableID\": [\"OffPeakConsumption\",\"PeakCons...</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T11:33:16.303598Z",
     "start_time": "2025-03-26T11:33:16.288569Z"
    }
   },
   "cell_type": "code",
   "source": "from etl.etl import *",
   "id": "efc0ef97630e082a",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T11:33:16.349636Z",
     "start_time": "2025-03-26T11:33:16.334622Z"
    }
   },
   "cell_type": "code",
   "source": [
    "all_prediction_columns = [\"PeakConsumption\", \"StandardConsumption\", \"OffPeakConsumption\",\"Block1Consumption\", \"Block2Consumption\",\"Block3Consumption\", \"Block4Consumption\",     \"NonTOUConsumption\"]\n",
    "\n",
    "metadata = extract_metadata(ufm_df)\n",
    "customer_ids = parse_json_column(ufm_df, \"CustomerJSON\")\n",
    "variable_ids = parse_json_column(ufm_df, \"varJSON\", key=\"VariableID\")\n",
    "columns_mapping = generate_combinations(all_prediction_columns)\n",
    "\n",
    "logging.info(f\"Customer IDs: {customer_ids}\")\n",
    "logging.info(f\"Variable IDs: {variable_ids}\")\n",
    "logging.info(f\"✅ Total column combinations: {len(columns_mapping)}\")"
   ],
   "id": "5b61d4443aa30238",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Generated 255 column combinations.\n",
      "INFO:root:Customer IDs: ['Klerksdorp', 'KSACS - Mmabatho', 'Kimberley', 'KSACS - Klerksdorp', 'KSACS - Soweto', 'South African Development Comm', 'Witbank', 'KSACS - Pretoria', 'Benoni', 'KSACS - Vereeniging', 'KSACS - Randfontein', 'KSACS - Benoni', 'KSACS - Nigel', 'KSACS - Witbank']\n",
      "INFO:root:Variable IDs: ['PeakConsumption', 'OffPeakConsumption', 'Block1Consumption', 'StandardConsumption']\n",
      "INFO:root:✅ Total column combinations: 255\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T11:33:16.427756Z",
     "start_time": "2025-03-26T11:33:16.419754Z"
    }
   },
   "cell_type": "code",
   "source": "selected_columns = find_matching_combination(columns_mapping, all_prediction_columns)",
   "id": "10f9932cbc2a8db7",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Exact match found for: frozenset({'NonTOUConsumption', 'Block1Consumption', 'StandardConsumption', 'Block2Consumption', 'PeakConsumption', 'Block3Consumption', 'Block4Consumption', 'OffPeakConsumption'})\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T11:33:16.505850Z",
     "start_time": "2025-03-26T11:33:16.491345Z"
    }
   },
   "cell_type": "code",
   "source": "from dml.dml import *",
   "id": "54da0f4de5a7cce7",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T11:33:16.816988Z",
     "start_time": "2025-03-26T11:33:16.539425Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df = load_and_prepare_data(path=None, ufmd=199, save=False, method=\"XGBoost\")\n",
    "df.head()\n"
   ],
   "id": "b73f7b586eb6c262",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:✅ Raw dataset loaded.\n",
      "INFO:root:✅ Raw dataset cleaned.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "                  PodID  CustomerID  TariffID  PeakConsumption  \\\n",
       "ReportingMonth                                                   \n",
       "2020-04-01      1873419  8460296087  MEGAFLEX         651020.0   \n",
       "2020-05-01      1873419  8460296087  MEGAFLEX         859170.0   \n",
       "2020-06-01      1873419  8460296087  MEGAFLEX         873638.0   \n",
       "2020-07-01      1873419  8460296087  MEGAFLEX        1025896.0   \n",
       "2020-08-01      1873419  8460296087  MEGAFLEX        1006638.0   \n",
       "\n",
       "                StandardConsumption  OffPeakConsumption  Block1Consumption  \\\n",
       "ReportingMonth                                                               \n",
       "2020-04-01                1926932.0           1981510.0                0.0   \n",
       "2020-05-01                2337768.0           3602408.0                0.0   \n",
       "2020-06-01                2127978.0           2858956.0                0.0   \n",
       "2020-07-01                2539960.0           3679766.0                0.0   \n",
       "2020-08-01                2522428.0           4198484.0                0.0   \n",
       "\n",
       "                Block2Consumption  Block3Consumption  Block4Consumption  \\\n",
       "ReportingMonth                                                            \n",
       "2020-04-01                    0.0                0.0                0.0   \n",
       "2020-05-01                    0.0                0.0                0.0   \n",
       "2020-06-01                    0.0                0.0                0.0   \n",
       "2020-07-01                    0.0                0.0                0.0   \n",
       "2020-08-01                    0.0                0.0                0.0   \n",
       "\n",
       "                NonTOUConsumption  \n",
       "ReportingMonth                     \n",
       "2020-04-01                    0.0  \n",
       "2020-05-01                    0.0  \n",
       "2020-06-01                    0.0  \n",
       "2020-07-01                    0.0  \n",
       "2020-08-01                    0.0  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PodID</th>\n",
       "      <th>CustomerID</th>\n",
       "      <th>TariffID</th>\n",
       "      <th>PeakConsumption</th>\n",
       "      <th>StandardConsumption</th>\n",
       "      <th>OffPeakConsumption</th>\n",
       "      <th>Block1Consumption</th>\n",
       "      <th>Block2Consumption</th>\n",
       "      <th>Block3Consumption</th>\n",
       "      <th>Block4Consumption</th>\n",
       "      <th>NonTOUConsumption</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ReportingMonth</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-04-01</th>\n",
       "      <td>1873419</td>\n",
       "      <td>8460296087</td>\n",
       "      <td>MEGAFLEX</td>\n",
       "      <td>651020.0</td>\n",
       "      <td>1926932.0</td>\n",
       "      <td>1981510.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-01</th>\n",
       "      <td>1873419</td>\n",
       "      <td>8460296087</td>\n",
       "      <td>MEGAFLEX</td>\n",
       "      <td>859170.0</td>\n",
       "      <td>2337768.0</td>\n",
       "      <td>3602408.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-01</th>\n",
       "      <td>1873419</td>\n",
       "      <td>8460296087</td>\n",
       "      <td>MEGAFLEX</td>\n",
       "      <td>873638.0</td>\n",
       "      <td>2127978.0</td>\n",
       "      <td>2858956.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-01</th>\n",
       "      <td>1873419</td>\n",
       "      <td>8460296087</td>\n",
       "      <td>MEGAFLEX</td>\n",
       "      <td>1025896.0</td>\n",
       "      <td>2539960.0</td>\n",
       "      <td>3679766.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-01</th>\n",
       "      <td>1873419</td>\n",
       "      <td>8460296087</td>\n",
       "      <td>MEGAFLEX</td>\n",
       "      <td>1006638.0</td>\n",
       "      <td>2522428.0</td>\n",
       "      <td>4198484.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T11:33:17.611734Z",
     "start_time": "2025-03-26T11:33:17.598528Z"
    }
   },
   "cell_type": "code",
   "source": "# df.to_csv(\"PredictiveInputDataXGBoost.csv\")",
   "id": "9fe333b875c4a7ce",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T11:33:17.845046Z",
     "start_time": "2025-03-26T11:33:17.833536Z"
    }
   },
   "cell_type": "code",
   "source": [
    "row = ufm_df.iloc[0]\n",
    "config = row_to_config(row)"
   ],
   "id": "4998ebf5f34f1526",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T11:33:18.062312Z",
     "start_time": "2025-03-26T11:33:18.053784Z"
    }
   },
   "cell_type": "code",
   "source": "config",
   "id": "ba13e398127d361f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ForecastConfig(forecast_method_id=6, forecast_method_name='XGBoost', model_parameters='(5,2,0.3,0.3,0.8)', region='EC', status='Failed', user_forecast_method_id=199, start_date=datetime.date(2025, 2, 1), end_date=datetime.date(2029, 1, 31), databrick_id=67)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T11:33:18.250559Z",
     "start_time": "2025-03-26T11:33:18.228237Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if df.empty:\n",
    "    logging.error(\"🚫 DataFrame is empty. Check input filters or data source.\")\n",
    "else:\n",
    "    customer_ids, pod_ids = get_unique_list_of_customer_and_pod(df)\n",
    "\n",
    "    # These variables should come from user input / config\n",
    "    StartDate = config.start_date\n",
    "    EndDate = config.end_date\n",
    "    Hyper_Parameters = config.model_parameters\n",
    "\n",
    "    forecast_dates = get_forecast_range(StartDate, EndDate)\n",
    "    params = extract_xgboost_params(Hyper_Parameters)\n",
    "    print(\"Param: \", params)\n",
    "    # Extract actuals range\n",
    "    latest_actual_date = df.index.max()\n",
    "    logging.info(f\"📍 Last actuals month in data: {latest_actual_date.strftime('%Y-%m')}\")"
   ],
   "id": "7a172a42d9a098ff",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:🧮 Forecasting for 2141\n",
      "INFO:root:📅 Forecast period: 2025-02-01 00:00:00 to 2029-01-01 00:00:00\n",
      "INFO:root:📍 Last actuals month in data: 2025-02\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Param:  (5.0, 2.0, 0.3, 0.3, 0.8)\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T11:33:18.842745Z",
     "start_time": "2025-03-26T11:33:18.471977Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.impute import KNNImputer, SimpleImputer\n",
    "from sklearn.metrics import mean_squared_error,r2_score,make_scorer\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, train_test_split, cross_val_score"
   ],
   "id": "8e8f0a9bd8b70d2d",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T11:33:18.874267Z",
     "start_time": "2025-03-26T11:33:18.864762Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Scaling the features\n",
    "scaler = StandardScaler()\n",
    "sim_imputer = SimpleImputer()\n",
    "consumption_types = [\"PeakConsumption\", \"StandardConsumption\", \"OffPeakConsumption\",\"Block1Consumption\", \"Block2Consumption\",\"Block3Consumption\", \"Block4Consumption\",     \"NonTOUConsumption\"]\n",
    "cons_types = [col for col in selected_columns if col in consumption_types]"
   ],
   "id": "bc88670d4e2941eb",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T11:33:18.920831Z",
     "start_time": "2025-03-26T11:33:18.906809Z"
    }
   },
   "cell_type": "code",
   "source": [
    "Forecast_Method_Name = \"XGBoost\"\n",
    "UFMID = 199\n",
    "DatabrickID = 67"
   ],
   "id": "651cde5057f8998c",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T11:33:18.999688Z",
     "start_time": "2025-03-26T11:33:18.994179Z"
    }
   },
   "cell_type": "code",
   "source": [
    "forecast_dates = pd.date_range(start=StartDate, end=EndDate, freq='MS')[0:]\n",
    "n_periods = len(forecast_dates)"
   ],
   "id": "502b1ccf66901084",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T11:33:19.232004Z",
     "start_time": "2025-03-26T11:33:19.221497Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from xg.utilities import *  \n",
    "import numpy as np"
   ],
   "id": "a0c85e08b3b83121",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T11:33:21.773894Z",
     "start_time": "2025-03-26T11:33:19.519554Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Main forecasting loop\n",
    "for customer_id in df['CustomerID'].unique():\n",
    "    #print(f\"Processing CustomerID: {customer_id}\")\n",
    "    customer_df = df[df[\"CustomerID\"] == customer_id]\n",
    "    if customer_df.empty:\n",
    "        #print(f\"No data found for CustomerID: {customer_id}\")\n",
    "        continue\n",
    "    unique_podel_ids = customer_df[\"PodID\"].unique()\n",
    "    for podel_id in unique_podel_ids:\n",
    "\n",
    "        # if podel_id == \"6582636837\":\n",
    "            RMSE_sum = 0\n",
    "            R2_sum = 0\n",
    "            #print(f\"Processing PODEL_ID: {podel_id}\")\n",
    "\n",
    "            podel_df = customer_df[customer_df[\"PodID\"] == podel_id]\n",
    "            if podel_df.empty:\n",
    "                #print(f\"No data found for PODEL_ID: {podel_id}\")\n",
    "                continue\n",
    "            future_predictions = []\n",
    "            forecast_df_all_cons = pd.DataFrame(forecast_dates, columns=[\"ReportingMonth\"])\n",
    "            forecast_df_all_cons[\"CustomerID\"] = customer_id\n",
    "            forecast_df_all_cons[\"PodID\"] = podel_id\n",
    "            forecast_df_all_cons[\"CustomerID\"] = forecast_df_all_cons[\"CustomerID\"].astype(int)\n",
    "            forecast_df_all_cons[\"PodID\"] = forecast_df_all_cons[\"PodID\"].astype(int)\n",
    "            forecast_df_all_cons[\"UserForecastMethodID\"] = UFMID    \n",
    "            performance_data = { 'ModelName': Forecast_Method_Name,\n",
    "                                'CustomerID': str(customer_id),\n",
    "                                'PodID': str(podel_id),\n",
    "                                'DataBrickID': int(DatabrickID),   \n",
    "                                'UserForecastMethodID': int(UFMID)\n",
    "                                }\n",
    "            historical_df = customer_df[customer_df[\"PodID\"] == podel_id].copy()            \n",
    "\n",
    "            # Create lag features for the historical data\n",
    "            # lag_columns = ['OffpeakConsumption', 'StandardConsumption', 'PeakConsumption']\n",
    "            lag_columns = selected_columns[2:]\n",
    "            #if debug:\n",
    "                #print(f'The lage columns are {lag_columns}')\n",
    "            podel_df = create_lag_features(podel_df, lag_columns, lags=3)\n",
    "\n",
    "            # Fill NaN values with 0 or an appropriate imputation method\n",
    "            for col in [f\"{col}_lag{lag}\" for col in lag_columns for lag in range(1, 4)]:\n",
    "                podel_df[col] = pd.to_numeric(podel_df[col], errors='coerce')\n",
    "            podel_df = podel_df.fillna(0)\n",
    "\n",
    "            feature_columns = [\"Month\", \"Year\"] + [f\"{col}_lag{lag}\" for col in lag_columns for lag in range(1, 4)]\n",
    "            for cons_type in cons_types:\n",
    "\n",
    "                # Prepare feature and target matrices\n",
    "                X = podel_df[feature_columns].values\n",
    "                Y = podel_df[cons_type].values\n",
    "\n",
    "                if podel_df[cons_type].isnull().all():\n",
    "                    #print(f\"No data found for consumption type: {cons_type}\")\n",
    "                    continue\n",
    "\n",
    "                #display( podel_df)\n",
    "                if X.shape[0] <= 5:\n",
    "                    continue\n",
    "                else:   \n",
    "                    try:\n",
    "                        X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "                        X_imputer = SimpleImputer(strategy='mean')\n",
    "                        Y_imputer = SimpleImputer(strategy='mean')\n",
    "                        \n",
    "                        # Impute missing values for X_train and X_test\n",
    "                        X_train_imputed = X_imputer.fit_transform(X_train)\n",
    "                        X_test_imputed = X_imputer.transform(X_test)\n",
    "\n",
    "                        # Scale the training features\n",
    "                        X_train_scaled = scaler.fit_transform(X_train_imputed)\n",
    "                        X_test_scaled = scaler.transform(X_test_imputed)\n",
    "\n",
    "                        # Impute missing values in Y_train and Y_test\n",
    "                        Y_train_imputed = Y_imputer.fit_transform(Y_train.reshape(-1,1))\n",
    "                        Y_test_imputed = Y_imputer.transform(Y_test.reshape(-1,1))\n",
    "\n",
    "\n",
    "                        # Define the parameter grid for XGBRegressor\n",
    "                        # param_grid = {\n",
    "                        #     'estimator__n_estimators': [10, 50, 100],\n",
    "                        #     'estimator__max_depth': [5, 10, 15],\n",
    "                        #     'estimator__learning_rate': [0.01, 0.1, 0.3],\n",
    "                        #     'estimator__subsample': [0.8, 0.9, 1],\n",
    "                        #     'estimator__colsample_bytree': [0.7, 0.8, 1]\n",
    "                        # }\n",
    "                        model = XGBRegressor(\n",
    "                            objective=\"reg:squarederror\",\n",
    "                            n_estimators=100,         # Number of boosting rounds\n",
    "                            max_depth=10,             # Maximum tree depth\n",
    "                            learning_rate=0.1,        # Learning rate\n",
    "                            subsample=0.9,            # Subsample ratio of training instances\n",
    "                            colsample_bytree=0.8,     # Subsample ratio of columns\n",
    "                            enable_categorical=True\n",
    "                        )\n",
    "\n",
    "                        # Fit the model to the training data\n",
    "                        model.fit(X_train_scaled, Y_train_imputed.ravel())  # Flatten Y_train to 1D\n",
    "\n",
    "                        # Make predictions on the test set\n",
    "                        Y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "                    except IndexError as e:\n",
    "                        print(f\"IndexError occurred,there is a mismatch in the no of elements : {e}\")\n",
    "                    except ValueError as e:\n",
    "                        print(f\"ValueError occurred: {e}\")\n",
    "                    except KeyError as e:\n",
    "                        print(f\"KeyError occurred, certain referenced column not found : {e}\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"An unexpected error occurred: {e}\")\n",
    "\n",
    "                    rmse = np.sqrt(mean_squared_error(Y_test_imputed, Y_pred))\n",
    "                    r2 = r2_score(Y_test_imputed, Y_pred)\n",
    "\n",
    "                    # if debug:\n",
    "                    #print(f\"Test RMSE: {rmse}\")\n",
    "                    #print(f\"Test R²: {r2}\")\n",
    "\n",
    "\n",
    "                    # model.fit(X_train_scaled, Y_train)\n",
    "\n",
    "                    # Prepare forecast dataframe\n",
    "                    forecast_df = pd.DataFrame(forecast_dates, columns=[\"ReportingMonth\"])\n",
    "                    forecast_df[\"CustomerID\"] = customer_id\n",
    "                    forecast_df[\"PodID\"] = podel_id\n",
    "                    forecast_df[\"Month\"] = forecast_df[\"ReportingMonth\"].dt.month\n",
    "                    forecast_df[\"Year\"] = forecast_df[\"ReportingMonth\"].dt.year\n",
    "                    forecast_df[\"CustomerID\"] = forecast_df[\"CustomerID\"].astype(int)\n",
    "                    forecast_df[\"PodID\"] = forecast_df[\"PodID\"].astype(int)\n",
    "                    forecast_df[\"UserForecastMethodID\"] = UFMID\n",
    "\n",
    "                    # Initialize lag columns in the forecast DataFrame\n",
    "                    for col in lag_columns:\n",
    "                        forecast_df[col] = np.nan  # Initialize the consumption columns\n",
    "                        for lag in range(1, 4):\n",
    "                            forecast_df[f\"{col}_lag{lag}\"] = np.nan\n",
    "\n",
    "                    # Main loop for predictions across each forecasted period\n",
    "                    for pred_cur_mth in range(n_periods):\n",
    "                        #print(f\"Prediction step {pred_cur_mth}\")\n",
    "\n",
    "                        # Update lag features for current step\n",
    "                        if pred_cur_mth == 0:\n",
    "                            # First step: Use historical data to initialize lags\n",
    "                            forecast_df = create_forecast_lag_features(forecast_df, podel_df, lag_columns, lags=3, step=pred_cur_mth)\n",
    "                            \n",
    "                        else:\n",
    "                            # Subsequent steps: Use previously predicted data to update lags\n",
    "                            forecast_df = create_forecast_lag_features(forecast_df, forecast_df, lag_columns, lags=3, step=pred_cur_mth)\n",
    "\n",
    "                        # Convert forecast_df columns to numeric types\n",
    "                        for col in [f\"{col}_lag{lag}\" for col in lag_columns for lag in range(1, 4)]:\n",
    "                            forecast_df[col] = pd.to_numeric(forecast_df[col], errors='coerce')\n",
    "\n",
    "                        # Select the current row of features for prediction\n",
    "                        X_forecast = forecast_df.loc[pred_cur_mth, feature_columns].values.reshape(1, -1)\n",
    "\n",
    "                        # Impute missing values in X_forecast\n",
    "                        X_forecast_imputed = X_imputer.transform(X_forecast)\n",
    "\n",
    "                        # Scale the features before making predictions\n",
    "                        X_forecast_scaled = scaler.transform(X_forecast_imputed)\n",
    "        \n",
    "                        # Make prediction\n",
    "                        prediction = model.predict(X_forecast_scaled)\n",
    "\n",
    "                        # Store the prediction\n",
    "                        future_predictions.append(prediction)\n",
    "\n",
    "\n",
    "                        # Update the forecast DataFrame with the predicted values\n",
    "                        if len(prediction.shape) == 1:\n",
    "                            # for idx, col in enumerate(selected_columns[2:]):\n",
    "                            #     if idx < len(prediction):\n",
    "                                    forecast_df.loc[pred_cur_mth, cons_type] = prediction\n",
    "                        else:\n",
    "                            # for idx, col in enumerate(selected_columns[2:]):\n",
    "                            #     if idx < prediction.shape[1]:\n",
    "                                    forecast_df.loc[pred_cur_mth, cons_type] = prediction[0]\n",
    "\n",
    "                        # Update lag features with the newly predicted values for future steps\n",
    "                        for lag in range(1, 4):\n",
    "                            next_step = pred_cur_mth + lag\n",
    "                            if next_step < len(forecast_df):\n",
    "                                # for col in selected_columns[2:]:\n",
    "                                    forecast_df.loc[next_step, f\"{cons_type}_lag{lag}\"] = forecast_df.loc[pred_cur_mth, cons_type]\n",
    "\n",
    "                        #print( forecast_df.loc[pred_cur_mth, cons_type])\n",
    "\n",
    "\n",
    "\n",
    "                    # if cons_type == \"PeakConsumption\":\n",
    "                    #     display(forecast_df)\n",
    "\n",
    "                    # #print(rename_dict)\n",
    "\n",
    "                    # #print(forecast_df.info())\n",
    "\n",
    "\n",
    "                lag_columns_to_drop = [f\"{col}_lag{lag}\" for col in lag_columns for lag in range(1, 4)]\n",
    "\n",
    "            \n",
    "                # Combine the specific columns to drop with the dynamic lag columns\n",
    "                columns_to_drop = [\"Month\", \"Year\"] + lag_columns_to_drop\n",
    "\n",
    "                # Drop the columns from the DataFrame\n",
    "                # forecast_df = forecast_df.drop(columns=columns_to_drop, axis=1)\n",
    "\n",
    "                    # display(forecast_df)\n",
    "                forecast_df_all_cons[cons_type]=forecast_df[cons_type]\n",
    "                performance_data[f\"RMSE_{cons_type}\"] = rmse\n",
    "                performance_data[f\"R2_{cons_type}\"] = r2\n",
    "\n",
    "\n",
    "                    # # Now plot historical vs forecasted values for the features of interest\n",
    "            \n",
    "                plot_forecast_vs_historical(historical_df, forecast_df,  [cons_type])  \n",
    "\n",
    "                        \n",
    "            for cons_type in cons_types:\n",
    "                if f\"RMSE_{cons_type}\" in performance_data and f\"R2_{cons_type}\" in performance_data:\n",
    "                    RMSE_sum +=  performance_data[f\"RMSE_{cons_type}\"]\n",
    "                    R2_sum   +=  performance_data[f\"R2_{cons_type}\"]\n",
    "\n",
    "            if len(cons_types) > 0:\n",
    "                rmse_avg = RMSE_sum/len(cons_types)\n",
    "                r2_avg = R2_sum/len(cons_types)\n",
    "\n",
    "\n",
    "            performance_data['RMSE_Avg'] = rmse_avg\n",
    "            performance_data['R2_Avg'] = r2_avg\n",
    "            performance_df = pd.DataFrame([performance_data])\n",
    " \n",
    "            forecast_combined_spark_df = forecast_combined_spark_df.withColumn(\"CustomerID\", forecast_combined_spark_df[\"CustomerID\"].cast(\"bigint\"))\n",
    "            forecast_combined_spark_df = forecast_combined_spark_df.withColumn(\"PodID\", forecast_combined_spark_df[\"PodID\"].cast(\"bigint\"))\n",
    "            forecast_combined_spark_df = forecast_combined_spark_df.withColumn(\"UserForecastMethodID\",forecast_combined_spark_df[\"UserForecastMethodID\"].cast(\"bigint\"))\n",
    "\n",
    "                                        # Write the DataFrame to the SQL table\n",
    "                # #print(forecast_df.columns)"
   ],
   "id": "b567512001caccf2",
   "outputs": [
    {
     "ename": "OverflowError",
     "evalue": "Python int too large to convert to C long",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mOverflowError\u001B[0m                             Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[18], line 24\u001B[0m\n\u001B[0;32m     22\u001B[0m forecast_df_all_cons[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCustomerID\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m customer_id\n\u001B[0;32m     23\u001B[0m forecast_df_all_cons[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPodID\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m podel_id\n\u001B[1;32m---> 24\u001B[0m forecast_df_all_cons[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCustomerID\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[43mforecast_df_all_cons\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mCustomerID\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mastype\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mint\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     25\u001B[0m forecast_df_all_cons[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPodID\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m forecast_df_all_cons[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPodID\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m.\u001B[39mastype(\u001B[38;5;28mint\u001B[39m)\n\u001B[0;32m     26\u001B[0m forecast_df_all_cons[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUserForecastMethodID\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m UFMID    \n",
      "File \u001B[1;32m~\\anaconda3\\envs\\data_science\\lib\\site-packages\\pandas\\core\\generic.py:6643\u001B[0m, in \u001B[0;36mNDFrame.astype\u001B[1;34m(self, dtype, copy, errors)\u001B[0m\n\u001B[0;32m   6637\u001B[0m     results \u001B[38;5;241m=\u001B[39m [\n\u001B[0;32m   6638\u001B[0m         ser\u001B[38;5;241m.\u001B[39mastype(dtype, copy\u001B[38;5;241m=\u001B[39mcopy, errors\u001B[38;5;241m=\u001B[39merrors) \u001B[38;5;28;01mfor\u001B[39;00m _, ser \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mitems()\n\u001B[0;32m   6639\u001B[0m     ]\n\u001B[0;32m   6641\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   6642\u001B[0m     \u001B[38;5;66;03m# else, only a single dtype is given\u001B[39;00m\n\u001B[1;32m-> 6643\u001B[0m     new_data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_mgr\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mastype\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcopy\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcopy\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43merrors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43merrors\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   6644\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_constructor_from_mgr(new_data, axes\u001B[38;5;241m=\u001B[39mnew_data\u001B[38;5;241m.\u001B[39maxes)\n\u001B[0;32m   6645\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\u001B[38;5;241m.\u001B[39m__finalize__(\u001B[38;5;28mself\u001B[39m, method\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mastype\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\data_science\\lib\\site-packages\\pandas\\core\\internals\\managers.py:430\u001B[0m, in \u001B[0;36mBaseBlockManager.astype\u001B[1;34m(self, dtype, copy, errors)\u001B[0m\n\u001B[0;32m    427\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m using_copy_on_write():\n\u001B[0;32m    428\u001B[0m     copy \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[1;32m--> 430\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    431\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mastype\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    432\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    433\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcopy\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcopy\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    434\u001B[0m \u001B[43m    \u001B[49m\u001B[43merrors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43merrors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    435\u001B[0m \u001B[43m    \u001B[49m\u001B[43musing_cow\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43musing_copy_on_write\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    436\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\data_science\\lib\\site-packages\\pandas\\core\\internals\\managers.py:363\u001B[0m, in \u001B[0;36mBaseBlockManager.apply\u001B[1;34m(self, f, align_keys, **kwargs)\u001B[0m\n\u001B[0;32m    361\u001B[0m         applied \u001B[38;5;241m=\u001B[39m b\u001B[38;5;241m.\u001B[39mapply(f, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    362\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 363\u001B[0m         applied \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mgetattr\u001B[39m(b, f)(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    364\u001B[0m     result_blocks \u001B[38;5;241m=\u001B[39m extend_blocks(applied, result_blocks)\n\u001B[0;32m    366\u001B[0m out \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mtype\u001B[39m(\u001B[38;5;28mself\u001B[39m)\u001B[38;5;241m.\u001B[39mfrom_blocks(result_blocks, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maxes)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\data_science\\lib\\site-packages\\pandas\\core\\internals\\blocks.py:758\u001B[0m, in \u001B[0;36mBlock.astype\u001B[1;34m(self, dtype, copy, errors, using_cow, squeeze)\u001B[0m\n\u001B[0;32m    755\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCan not squeeze with more than one column.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    756\u001B[0m     values \u001B[38;5;241m=\u001B[39m values[\u001B[38;5;241m0\u001B[39m, :]  \u001B[38;5;66;03m# type: ignore[call-overload]\u001B[39;00m\n\u001B[1;32m--> 758\u001B[0m new_values \u001B[38;5;241m=\u001B[39m \u001B[43mastype_array_safe\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvalues\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcopy\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcopy\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43merrors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43merrors\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    760\u001B[0m new_values \u001B[38;5;241m=\u001B[39m maybe_coerce_values(new_values)\n\u001B[0;32m    762\u001B[0m refs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\data_science\\lib\\site-packages\\pandas\\core\\dtypes\\astype.py:237\u001B[0m, in \u001B[0;36mastype_array_safe\u001B[1;34m(values, dtype, copy, errors)\u001B[0m\n\u001B[0;32m    234\u001B[0m     dtype \u001B[38;5;241m=\u001B[39m dtype\u001B[38;5;241m.\u001B[39mnumpy_dtype\n\u001B[0;32m    236\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 237\u001B[0m     new_values \u001B[38;5;241m=\u001B[39m \u001B[43mastype_array\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvalues\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcopy\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcopy\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    238\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m (\u001B[38;5;167;01mValueError\u001B[39;00m, \u001B[38;5;167;01mTypeError\u001B[39;00m):\n\u001B[0;32m    239\u001B[0m     \u001B[38;5;66;03m# e.g. _astype_nansafe can fail on object-dtype of strings\u001B[39;00m\n\u001B[0;32m    240\u001B[0m     \u001B[38;5;66;03m#  trying to convert to float\u001B[39;00m\n\u001B[0;32m    241\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m errors \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mignore\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\data_science\\lib\\site-packages\\pandas\\core\\dtypes\\astype.py:182\u001B[0m, in \u001B[0;36mastype_array\u001B[1;34m(values, dtype, copy)\u001B[0m\n\u001B[0;32m    179\u001B[0m     values \u001B[38;5;241m=\u001B[39m values\u001B[38;5;241m.\u001B[39mastype(dtype, copy\u001B[38;5;241m=\u001B[39mcopy)\n\u001B[0;32m    181\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 182\u001B[0m     values \u001B[38;5;241m=\u001B[39m \u001B[43m_astype_nansafe\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvalues\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcopy\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcopy\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    184\u001B[0m \u001B[38;5;66;03m# in pandas we don't store numpy str dtypes, so convert to object\u001B[39;00m\n\u001B[0;32m    185\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(dtype, np\u001B[38;5;241m.\u001B[39mdtype) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28missubclass\u001B[39m(values\u001B[38;5;241m.\u001B[39mdtype\u001B[38;5;241m.\u001B[39mtype, \u001B[38;5;28mstr\u001B[39m):\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\data_science\\lib\\site-packages\\pandas\\core\\dtypes\\astype.py:133\u001B[0m, in \u001B[0;36m_astype_nansafe\u001B[1;34m(arr, dtype, copy, skipna)\u001B[0m\n\u001B[0;32m    129\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(msg)\n\u001B[0;32m    131\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m copy \u001B[38;5;129;01mor\u001B[39;00m arr\u001B[38;5;241m.\u001B[39mdtype \u001B[38;5;241m==\u001B[39m \u001B[38;5;28mobject\u001B[39m \u001B[38;5;129;01mor\u001B[39;00m dtype \u001B[38;5;241m==\u001B[39m \u001B[38;5;28mobject\u001B[39m:\n\u001B[0;32m    132\u001B[0m     \u001B[38;5;66;03m# Explicit copy, or required since NumPy can't view from / to object.\u001B[39;00m\n\u001B[1;32m--> 133\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43marr\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mastype\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcopy\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m    135\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m arr\u001B[38;5;241m.\u001B[39mastype(dtype, copy\u001B[38;5;241m=\u001B[39mcopy)\n",
      "\u001B[1;31mOverflowError\u001B[0m: Python int too large to convert to C long"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "b7979ff45819da9e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "65e9a379c28bc2f8"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_science",
   "language": "python",
   "name": "data_science"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
